# -*- coding: utf-8 -*-
"""STROKE_MODEL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KnJmfANsfz3oKDx8pAfiuCnY_QJO_AqD
"""

import gzip
import pickle
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import RandomOverSampler
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix , ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

data = pd.read_csv("/media/stroke.csv")

data.head()

data.drop('id', axis=1, inplace=True)
data.shape

data.drop_duplicates()

data.info()

data[['hypertension', 'heart_disease', 'stroke']] = data[['hypertension', 'heart_disease', 'stroke']].astype(str)

data.info()

data.describe()

fig, ax = plt.subplots(3, 1, figsize = (20, 15))
plt.suptitle('Distribution of Numerical features based on target variable', fontsize = 25, color = 'teal')
sns.histplot(x = data['age'], hue= data['stroke'], kde= True, ax= ax[0], palette = 'ocean')
ax[0].set(xlabel = 'Age')
sns.histplot(x = data['avg_glucose_level'], hue= data['stroke'], kde= True, ax= ax[1], palette = 'twilight')
ax[1].set(xlabel = 'Average Glucose Level')
sns.histplot(x = data['bmi'], hue= data['stroke'], kde= True, ax= ax[2], palette = 'viridis')
ax[2].set(xlabel = 'Body Mass Index')
plt.show()

for col in ['avg_glucose_level', 'bmi']:
    data[col] = np.log(data[col])

data.head()

data.describe()

sns.countplot(x = data['stroke'], palette= 'winter')
plt.xlabel('Stroke');

cat_features = data.select_dtypes(exclude="number").columns

cat_cols = cat_features[:-1]
for col in cat_cols:
    print(f'============{col}============\n {data[col].value_counts()}\n')

data.drop(data[data['gender'] == 'Other'].index, inplace= True)

fig,ax = plt.subplots(7,2,figsize=(18,35))
for i, col in enumerate(cat_cols):
    sns.countplot(data = data, x = col, ax=ax[i,0])
    sns.countplot(data = data, x = col,hue='stroke', ax=ax[i,1])
    if i == 0:
        ax[0,0].set_title('Count plot for Categorical Features', loc='center', y=1.1, size=18, weight='bold',color='green')
    else:
        ax[0,1].set_title('Count plot for Categorical Features Based on Stroke', loc='center', y=1.1, size=18, weight='bold',color='green')

data.isnull().sum()

def knn_impute(df, na_target):
    df = df.copy()

    numeric_df = df.select_dtypes(np.number)
    non_na_columns = numeric_df.loc[:, numeric_df.isna().sum() == 0].columns

    y_train = numeric_df.loc[numeric_df[na_target].isna() == False, na_target]
    X_train = numeric_df.loc[numeric_df[na_target].isna() == False, non_na_columns]
    X_test = numeric_df.loc[numeric_df[na_target].isna() == True, non_na_columns]

    knn = KNeighborsRegressor()
    knn.fit(X_train, y_train)

    y_pred = knn.predict(X_test)

    df.loc[df[na_target].isna() == True, na_target] = y_pred

    return df

data1 = knn_impute(data, 'bmi')
data1.isnull().sum()

data2 = pd.get_dummies(data1, drop_first= True)
data2.head()

data2.shape

data2.describe()

# # lưu tập dữ liệu để sử dụng cho chuẩn hóa dữ liệu đầu vào
# data2[['age','avg_glucose_level','bmi']].to_csv('standardScaler.csv', index=False)

s = StandardScaler()
data2[['bmi', 'avg_glucose_level', 'age']] = s.fit_transform(data2[['bmi', 'avg_glucose_level', 'age']])

s.mean_

s.scale_

data3 = data2.copy()
data3.head()

oversample = RandomOverSampler(sampling_strategy='minority')
X = data3.drop(['stroke_1'], axis=1)
y = data3['stroke_1']
X_over, y_over = oversample.fit_resample(X, y)

X_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size= 0.1, random_state= 42)

sns.countplot(x = y_train, palette= 'winter')
plt.xlabel('Stroke');

print(y_train.value_counts())
print(y_test.value_counts())

parameters = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    'gamma': [0.001, 0.01, 0.1, 1, 100, 1000]
}

svm = RandomizedSearchCV(SVC(probability= True), parameters, cv=5)
svm.fit(X_train, y_train)
svm.best_params_

SVM = SVC(probability=True, C=10, gamma=1000)
SVM.fit(X_train, y_train)

y_pred_svm = SVM.predict(X_test)
y_pred_prob_svm = SVM.predict_proba(X_test)[:, 1]

print('Accuracy:', accuracy_score(y_test, y_pred_svm))
print('ROC AUC Score:', roc_auc_score(y_test, y_pred_prob_svm))

cm = confusion_matrix(y_test, y_pred_svm)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.show()

rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train,y_train)
y_pred_rf = rf.predict(X_test)
y_pred_prob_rf = rf.predict_proba(X_test)[:, 1]

print('Accuracy:', accuracy_score(y_test, y_pred_rf))
print('ROC AUC Score:', roc_auc_score(y_test, y_pred_prob_rf))
cm = confusion_matrix(y_test, y_pred_rf)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.show()

new_data = np.array([[0.876362,0.916200,0.479036, 1,0,0,1,0,1,0,0,0,1,0,0]])

s = SVM.predict(new_data)
if s == 0:
  print("Khong benh")
else:
  print("Co benh")

# with gzip.open('.\\Model\\svm_stroke_model.pkl.gz', 'wb') as f:
#     pickle.dump(SVM, f)